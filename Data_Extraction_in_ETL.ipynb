{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f8GFansSump"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : Describe different types of data sources used in ETL with suitable examples\n",
        "\n",
        "\n",
        "```\n",
        "ETL (Extract, Transform, Load) process uses multiple types of data sources depending on business needs. These data sources can be broadly classified as follows:\n",
        "\n",
        "1. Database Sources\n",
        "These are structured data sources stored in databases.\n",
        "\n",
        "ðŸ”¹ Examples:\n",
        "   > Relational Databases: MySQL, PostgreSQL, Oracle, SQL Server.\n",
        "   > NoSQL Databases: MongoDB, Cassandra.\n",
        "\n",
        "ðŸ”¹ Use Case:\n",
        "    > Customer records stored in a MySQL database.\n",
        "    > Sales transactions stored in Oracle DB.\n",
        "\n",
        "\n",
        "2. Flat File Sources\n",
        "Data stored in file formats, often outside databases.\n",
        "\n",
        "ðŸ”¹ Examples:\n",
        "    > CSV (Comma Separated Values).\n",
        "    > Excel files (.xls, .xlsx).\n",
        "    > Text files (.txt).\n",
        "    > JSON and XML files.\n",
        "\n",
        "ðŸ”¹ Use Case:\n",
        "    > Daily sales report in CSV.\n",
        "    > Employee details maintained in Excel.\n",
        "\n",
        "3. Application Sources\n",
        "Data generated by business applications.\n",
        "\n",
        "ðŸ”¹ Examples:\n",
        "    > ERP systems (SAP).\n",
        "    > CRM systems (Salesforce).\n",
        "    > HR systems (Workday).\n",
        "\n",
        "ðŸ”¹ Use Case:\n",
        "    > Customer interaction data from Salesforce.\n",
        "    > Inventory data from SAP.\n",
        "\n",
        "4. Web Services / APIs\n",
        "Data collected from external or internal APIs.\n",
        "\n",
        "ðŸ”¹ Examples:\n",
        "    > REST APIs\n",
        "    > SOAP APIs\n",
        "\n",
        "ðŸ”¹ Use Case:\n",
        "    > Weather data from an external REST API\n",
        "    > Payment data from a payment gateway API\n",
        "\n",
        "```\n",
        "Question 2 : What is data extraction? Explain its role in the ETL pipeline.\n",
        "\n",
        "\n",
        "```\n",
        "Data Extraction is the process of collecting raw data from various source systems such as databases, files, APIs, or applications for further processing in the ETL pipeline.\n",
        "\n",
        "It ensures that the required data is correctly and efficiently pulled from the source systems.\n",
        "\n",
        "Role of Data Extraction in the ETL Pipeline\n",
        "   1.Starting Point of ETL\n",
        "   Data extraction is the foundation of the ETL pipeline. If incorrect or incomplete data is extracted, the entire ETL process will produce unreliable results.\n",
        "\n",
        "   2.Collects Data from Multiple Sources\n",
        "   Extraction gathers data from different systems such as:\n",
        "     # Relational databases (MySQL, Oracle)\n",
        "     # Flat files (CSV, Excel)\n",
        "     # APIs and web services\n",
        "     # Cloud storage systems\n",
        "\n",
        "   3. Ensures Data Availability for Transformation\n",
        "       Extracted data is made available for cleaning, validation, and transformation before loading into the target system.\n",
        "\n",
        "   4. Supports Full and Incremental Loads\n",
        "       # Full extraction: All data is extracted at once\n",
        "       # Incremental extraction: Only new or modified data is extracted\n",
        "        This improves performance and reduces system load.\n",
        "    5. Maintains Source System Performance\n",
        "       Efficient extraction minimizes impact on operational systems by scheduling jobs during low-usage periods or using optimized queries.\n",
        "```\n",
        "Question 3 : Explain the difference between CSV and Excel in terms of extraction and ETL usage.\n",
        "\n",
        "\n",
        "```\n",
        "CSV and Excel are commonly used flat file data sources in ETL processes. However, they differ significantly in structure, extraction complexity, and ETL usage.\n",
        "\n",
        "1. File Format and Structure\n",
        "    > CSV (Comma-Separated Values):\n",
        "    A plain text file where data values are separated by commas. It contains only raw data with no formatting.\n",
        "\n",
        "    > Excel:\n",
        "    A binary or XML-based file format (.xls, .xlsx) that can contain multiple sheets, formulas, formatting, and charts.\n",
        "2. Ease of Data Extraction\n",
        "    > CSV:\n",
        "    Easy to extract because of its simple and consistent structure. Most ETL tools can read CSV files directly with minimal configuration.\n",
        "\n",
        "    > Excel:\n",
        "    More complex to extract due to multiple sheets, merged cells, formulas, and formatting. ETL tools require additional configuration to select specific sheets and ranges.\n",
        "\n",
        "```\n",
        "Question 4 : Explain the steps involved in extracting data from a relational database.\n",
        "\n",
        "\n",
        "```\n",
        "> Identify the source database\n",
        "  Select the relational database (e.g., MySQL, Oracle, SQL Server) and understand its schema, tables, and relationships.\n",
        "\n",
        "> Define data requirements\n",
        "  Decide which tables, columns, and records are required, and whether the extraction will be full or incremental.\n",
        "\n",
        "> Establish database connection\n",
        "  Connect to the database using proper credentials and JDBC/ODBC drivers.\n",
        "\n",
        "> Write SQL queries\n",
        "  Create SELECT queries with necessary filters, joins, and conditions to retrieve accurate data.\n",
        "\n",
        "> Execute data extraction\n",
        "  Run the queries to extract data, often in batches for large datasets to improve performance.\n",
        "\n",
        "> Apply incremental extraction (if needed)\n",
        "  Extract only new or updated records using timestamps, primary keys, or change data capture (CDC).\n",
        "\n",
        "> Validate extracted data\n",
        "  Verify data completeness, accuracy, and consistency with the source system.\n",
        "\n",
        "> Store data in a staging area\n",
        "  Save the extracted data temporarily for further transformation and loading.\n",
        "```\n",
        "Question 5 : Explain three common challenges faced during data extraction.\n",
        "\n",
        "\n",
        "```\n",
        "> Data Quality Issues\n",
        "  Source data may contain missing values, duplicates, inconsistent formats, or incorrect entries. These issues can lead to inaccurate extraction results and affect the quality of data used in later ETL stages.\n",
        "\n",
        "> Performance Impact on Source Systems\n",
        "  Extracting large volumes of data can slow down operational databases, especially during peak business hours. Poorly optimized queries may increase load and affect system performance.\n",
        "\n",
        "> Data Format and Schema Changes\n",
        "  Changes in source database structure, such as added or modified columns, data type changes, or file format variations, can break extraction processes and cause failures in the ETL pipeline.\n",
        "```\n",
        "Question 6 : What are APIs? Explain how APIs help in real-time data extraction.\n",
        "\n",
        "```\n",
        "APIs (Application Programming Interfaces) are sets of rules and protocols that allow different software applications to communicate with each other. APIs define how requests for data are made and how responses are returned, usually in formats like JSON or XML.\n",
        "\n",
        "How APIs Help in Real-Time Data Extraction\n",
        "  > Direct Access to Live Data\n",
        "    APIs provide direct access to data generated by applications in real time, such as user activities, transactions, or sensor data.\n",
        "\n",
        "  > Event-Based Data Retrieval\n",
        "    Many APIs support event-driven or streaming mechanisms, allowing systems to receive data immediately when an event occurs.\n",
        "\n",
        "  > Standardized Data Exchange\n",
        "    APIs use standard request and response formats, making it easier to extract and integrate real-time data from different systems.\n",
        "\n",
        "  > Secure Data Access\n",
        "    APIs support authentication and authorization methods (such as API keys or tokens), ensuring secure real-time data extraction.\n",
        "\n",
        "  > Scalability and Automation\n",
        "    APIs enable automated and scalable data extraction without manual intervention, which is essential for continuous real-time ETL processes.\n",
        "```\n",
        "Question 7 : Why are databases preferred for enterprise-level data extraction?\n",
        "\n",
        "\n",
        "```\n",
        "> High Data Reliability\n",
        "  Databases ensure data accuracy, consistency, and integrity through constraints, transactions, and validation rules.\n",
        "\n",
        "> Efficient Handling of Large Data Volumes\n",
        "  Relational databases are designed to store and process massive amounts of data efficiently.\n",
        "\n",
        "> Structured Data Organization\n",
        "  Data is stored in well-defined tables with clear relationships, making extraction easier and more systematic.\n",
        "\n",
        "> Support for Complex Queries\n",
        "  Databases allow advanced SQL queries, joins, and aggregations to extract precise and meaningful data.\n",
        "\n",
        "> Security and Access Control\n",
        "  Databases provide strong security features such as authentication, authorization, and role-based access control.\n",
        "\n",
        "> Support for Incremental and Real-Time Extraction\n",
        "   Features like timestamps, triggers, and Change Data Capture (CDC) enable incremental and near real-time data extraction.\n",
        "\n",
        "> Scalability and Performance Optimization\n",
        "  Indexing, partitioning, and query optimization techniques help maintain high performance as data grows.\n",
        "\n",
        "> Integration with ETL Tools\n",
        "  Most enterprise ETL tools natively support database connections, making automation and scheduling easier.\n",
        "```\n",
        "Question 8 : What steps should an ETL developer take when extracting data from large CSV files (1GB+)?\n",
        "\n",
        "```\n",
        "> Use Streaming or Chunk-Based Processing\n",
        "  Read the CSV file in chunks instead of loading the entire file into memory to avoid memory overflow.\n",
        "\n",
        "> Validate File Structure Before Processing\n",
        "  Check delimiters, headers, encoding, and column consistency to prevent extraction failures.\n",
        "\n",
        "> Apply Filters Early\n",
        "  Extract only required columns and rows to reduce processing time and resource usage.\n",
        "\n",
        "> Handle Data Types and Null Values Properly\n",
        "  Define data types explicitly and manage missing or invalid values during extraction.\n",
        "\n",
        "> Enable Parallel Processing (If Possible)\n",
        "   Split the file into parts or use parallel threads to speed up extraction.\n",
        "\n",
        "> Use Compression and Efficient Storage\n",
        "   Work with compressed files or convert data into optimized formats to improve I/O performance.\n",
        "\n",
        "> Log and Monitor the Extraction Process\n",
        "  Track progress, errors, and performance metrics to quickly identify and fix issues.\n",
        "\n",
        "> Test with Sample Data First\n",
        "  Validate extraction logic on smaller samples before processing the full 1GB+ file.\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jYWWXxrkSvkj"
      }
    }
  ]
}